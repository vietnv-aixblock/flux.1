from dataclasses import dataclass
from typing import Optional


@dataclass
class TrainingConfigFlux:
    pretrained_model_name_or_path: str
    instance_prompt: str
    revision: Optional[str] = None
    variant: Optional[str] = None
    dataset_name: Optional[str] = None
    dataset_config_name: Optional[str] = None
    instance_data_dir: Optional[str] = None
    cache_dir: Optional[str] = None
    image_column: str = "image"
    caption_column: Optional[str] = None
    repeats: int = 1
    class_data_dir: Optional[str] = None
    class_prompt: Optional[str] = None
    max_sequence_length: int = 77
    validation_prompt: Optional[str] = None
    num_validation_images: int = 4
    validation_epochs: int = 50
    with_prior_preservation: bool = False
    prior_loss_weight: float = 1.0
    num_class_images: int = 100
    output_dir: str = "flux-dreambooth"
    seed: Optional[int] = None
    resolution: int = 512
    center_crop: bool = False
    random_flip: bool = False
    train_text_encoder: bool = False
    train_batch_size: int = 1
    sample_batch_size: int = 1
    num_train_epochs: int = 1
    max_train_steps: Optional[int] = None
    checkpointing_steps: int = 500
    checkpoints_total_limit: Optional[int] = None
    resume_from_checkpoint: Optional[str] = None
    gradient_accumulation_steps: int = 1
    gradient_checkpointing: bool = False
    learning_rate: float = 1e-4
    guidance_scale: float = 3.5
    text_encoder_lr: float = 5e-6
    scale_lr: bool = False
    lr_scheduler: str = "constant"
    lr_warmup_steps: int = 500
    lr_num_cycles: int = 1
    lr_power: float = 1.0
    dataloader_num_workers: int = 0
    weighting_scheme: str = "none"
    logit_mean: float = 0.0
    logit_std: float = 1.0
    mode_scale: float = 1.29
    optimizer: str = "AdamW"
    use_8bit_adam: bool = False
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    prodigy_beta3: Optional[float] = None
    prodigy_decouple: bool = True
    adam_weight_decay: float = 1e-4
    adam_weight_decay_text_encoder: float = 1e-3
    adam_epsilon: float = 1e-8
    prodigy_use_bias_correction: bool = True
    prodigy_safeguard_warmup: bool = True
    max_grad_norm: float = 1.0
    push_to_hub: bool = False
    hub_token: Optional[str] = None
    hub_model_id: Optional[str] = None
    logging_dir: str = "logs"
    allow_tf32: bool = False
    report_to: str = "tensorboard"
    mixed_precision: Optional[str] = None
    prior_generation_precision: Optional[str] = None
    local_rank: int = -1
    channel_log: Optional[str] = None
    clip_l: Optional[str] = None
    t5xxl: Optional[str] = None
    vae: Optional[str] = None
    disable_mmap_load_safetensors: bool = False


@dataclass
class TrainingConfigFluxLora:
    pretrained_model_name_or_path: str
    instance_prompt: str
    revision: Optional[str] = None
    variant: Optional[str] = None
    dataset_name: Optional[str] = None
    dataset_config_name: Optional[str] = None
    instance_data_dir: Optional[str] = None
    cache_dir: Optional[str] = None
    image_column: str = "image"
    caption_column: Optional[str] = None
    repeats: int = 1
    class_data_dir: Optional[str] = None
    class_prompt: Optional[str] = None
    max_sequence_length: int = 512
    validation_prompt: Optional[str] = None
    num_validation_images: int = 4
    validation_epochs: int = 50
    rank: int = 4
    with_prior_preservation: bool = False
    prior_loss_weight: float = 1.0
    num_class_images: int = 100
    output_dir: str = "flux-dreambooth-lora"
    seed: Optional[int] = None
    resolution: int = 512
    center_crop: bool = False
    random_flip: bool = False
    train_text_encoder: bool = False
    train_batch_size: int = 1
    sample_batch_size: int = 1
    num_train_epochs: int = 1
    max_train_steps: Optional[int] = None
    checkpointing_steps: int = 500
    checkpoints_total_limit: Optional[int] = None
    resume_from_checkpoint: Optional[str] = None
    gradient_accumulation_steps: int = 1
    gradient_checkpointing: bool = False
    learning_rate: float = 1e-4
    guidance_scale: float = 3.5
    text_encoder_lr: float = 5e-6
    scale_lr: bool = False
    lr_scheduler: str = "constant"
    lr_warmup_steps: int = 500
    lr_num_cycles: int = 1
    lr_power: float = 1.0
    dataloader_num_workers: int = 0
    weighting_scheme: str = "none"
    logit_mean: float = 0.0
    logit_std: float = 1.0
    mode_scale: float = 1.29
    optimizer: str = "AdamW"
    use_8bit_adam: bool = False
    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    prodigy_beta3: Optional[float] = None
    prodigy_decouple: bool = True
    adam_weight_decay: float = 1e-4
    adam_weight_decay_text_encoder: float = 1e-3
    lora_layers: Optional[str] = None
    adam_epsilon: float = 1e-8
    prodigy_use_bias_correction: bool = True
    prodigy_safeguard_warmup: bool = True
    max_grad_norm: float = 1.0
    push_to_hub: bool = False
    hub_token: Optional[str] = None
    hub_model_id: Optional[str] = None
    logging_dir: str = "logs"
    allow_tf32: bool = False
    cache_latents: bool = False
    report_to: str = "tensorboard"
    mixed_precision: Optional[str] = None
    upcast_before_saving: bool = False
    prior_generation_precision: Optional[str] = None
    local_rank: int = -1
    training_args_json: Optional[str] = None
    channel_log: Optional[str] = None
    clip_l: Optional[str] = None
    t5xxl: Optional[str] = None
    vae: Optional[str] = None
    disable_mmap_load_safetensors: bool = False
