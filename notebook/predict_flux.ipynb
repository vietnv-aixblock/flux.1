{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "from diffusers import DiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt setup\n",
    "\n",
    "prompt = \"a cat sitting on a pole\"\n",
    "model_id = \"black-forest-labs/FLUX.1-dev\"\n",
    "width = 64\n",
    "height = 64\n",
    "num_inference_steps = 1\n",
    "guidance_scale = 2\n",
    "format = \"JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpu\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# check gpu(s)\n",
    "n_gpus = torch.cuda.device_count()\n",
    "try:\n",
    "    _ = f\"{int(torch.cuda.mem_get_info()[0] / 1024 ** 3) - 2}GB\"\n",
    "except AssertionError:\n",
    "    _ = 0\n",
    "max_memory = {i: _ for i in range(n_gpus)}\n",
    "print('max memory:', max_memory)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe setup\n",
    "\n",
    "compute_capability = torch.cuda.get_device_properties(0).major\n",
    "if compute_capability > 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "elif compute_capability>7:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None  # auto setup for < 7\n",
    "\n",
    "try:\n",
    "    pipe = FluxPipeline.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
    "except Exception as e:\n",
    "    base_model = \"black-forest-labs/FLUX.1-dev\"\n",
    "    pipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch_dtype)\n",
    "    pipe.load_lora_weights(model_id)\n",
    "\n",
    "\n",
    "# # for low GPU RAM, quantize from 16b to 8b\n",
    "# quantize(pipe.transformer, weights=qfloat8)\n",
    "# freeze(pipe.transformer)\n",
    "# quantize(pipe.text_encoder_2, weights=qfloat8)\n",
    "# freeze(pipe.text_encoder_2)\n",
    "\n",
    "# # for even lower GPU RAM\n",
    "# pipe.vae.enable_tiling()\n",
    "# pipe.vae.enable_slicing()\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=torch.Generator(device=device)\n",
    ").images[0]\n",
    "\n",
    "\n",
    "pipe = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the result\n",
    "\n",
    "# refs\n",
    "# predictions.append({\n",
    "#                     'result': [{\n",
    "#                         'from_name': \"generated_text\",\n",
    "#                         'to_name': \"text_output\", #audio\n",
    "#                         'type': 'textarea',\n",
    "#                         'value': {\n",
    "#                             'data': base64_output,\n",
    "#                             \"url\": generated_url, \n",
    "#                         }\n",
    "#                     }],\n",
    "#                     'model_version': \"\"\n",
    "#                 })\n",
    "#                 print(predictions)\n",
    "\n",
    "\n",
    "buffered = BytesIO()\n",
    "image.save(buffered, format=format)\n",
    "img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "result = {\n",
    "  \"model_version\": model_id,\n",
    "  \"result\":{\n",
    "    \"format\": format,\n",
    "    \"image\": img_base64,\n",
    "    # \"image_url\": //TODO: store in s3 bucket\n",
    "  },\n",
    "}\n",
    "\n",
    "json_response = {\"message\": \"predict completed successfully\", \"result\": result}\n",
    "\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
